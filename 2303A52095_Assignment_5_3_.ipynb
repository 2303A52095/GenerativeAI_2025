{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1R+UMyY7JFB7ertGc9f4r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52095/GenerativeAI_2025/blob/main/2303A52095_Assignment_5_3_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. (1 ponto) Design a multi-layer ANN architecture with one input, one hidden, and one output\n",
        "layer. Assume a linear activation function in the output layer and a sigmoid activation function\n",
        "in the hidden layer.\n",
        "• Write Python code for a backpropagation algorithm with gradient descent optimization to\n",
        "update weights and bias parameters of the ANN model with training data shown in Table\n",
        "1.\n",
        "• Calculate the mean square error with training and testing data shown in Table 2.\n",
        "• Write Python code that reads the input data [x1 and x2] from the user. Predict the output\n",
        "with deployed ANN model\n",
        "Table 1: Training Data\n",
        "\n",
        "x1 x2 y\n",
        "\n",
        "0.2 0.1 0.3441\n",
        "\n",
        "0.3 0.2 0.3500\n",
        "\n",
        "0.4 0.3 0.3558\n",
        "\n",
        "0.7 0.6 0.3729\n",
        "\n",
        "0.8 0.7 0.3785\n",
        "\n",
        "0.9 0.8 0.3841\n",
        "\n",
        "Tabela 2: Test Data\n",
        "\n",
        "x1 x2 y\n",
        "\n",
        "0.5 0.4 0.3615\n",
        "\n",
        "0.6 0.5 0.3672\n",
        "\n",
        "• Expected learning Outcomes from this assignment related to python\\\n",
        "\n",
        "– Students are able to understand how backpropagation algorithm helps to update model\n",
        "parameters of multilayer ANN\n",
        "\n",
        "– Students are able to write code in python for backpropagation algorithm\n",
        "\n",
        "– Students are able to design architecture of ANN based on problem statement\n",
        "\n",
        "– Students are able to derive mathematical expression for change in weights and bias\n",
        "parameters for different activation functions\n",
        "\n"
      ],
      "metadata": {
        "id": "ISjf4ywyLZ8W"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "def linear_derivative(x):\n",
        "    return 1\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self):  # Corrected constructor name to __init__\n",
        "        self.weights1 = np.random.uniform(size=(2, 3))\n",
        "        self.weights2 = np.random.uniform(size=(3, 1))\n",
        "        self.bias1 = np.zeros((1, 3))\n",
        "        self.bias2 = np.zeros((1, 1))\n",
        "        self.learning_rate = 0.1\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input = X\n",
        "        self.hidden_input = np.dot(X, self.weights1) + self.bias1\n",
        "        self.hidden_output = sigmoid(self.hidden_input)\n",
        "        self.output = np.dot(self.hidden_output, self.weights2) + self.bias2\n",
        "        self.final_output = linear(self.output)\n",
        "        return self.final_output\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        self.output_error = y - output\n",
        "        self.output_delta = self.output_error * linear_derivative(output)\n",
        "\n",
        "        self.hidden_error = np.dot(self.output_delta, self.weights2.T)\n",
        "        self.hidden_delta = self.hidden_error * sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        self.weights2 += self.learning_rate * np.dot(self.hidden_output.T, self.output_delta)\n",
        "        self.bias2 += self.learning_rate * np.sum(self.output_delta, axis=0, keepdims=True)\n",
        "        self.weights1 += self.learning_rate * np.dot(X.T, self.hidden_delta)\n",
        "        self.bias1 += self.learning_rate * np.sum(self.hidden_delta, axis=0, keepdims=True)\n",
        "\n",
        "    def train(self, X, y, epochs=1000):\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "\n",
        "# Training and testing data (unchanged)\n",
        "train_data = np.array([\n",
        "    [0.2, 0.1],\n",
        "    [0.3, 0.2],\n",
        "    [0.4, 0.3],\n",
        "    [0.7, 0.6],\n",
        "    [0.8, 0.7],\n",
        "    [0.9, 0.8]\n",
        "])\n",
        "\n",
        "train_target = np.array([\n",
        "    [0.3441],\n",
        "    [0.3500],\n",
        "    [0.3558],\n",
        "    [0.3729],\n",
        "    [0.3785],\n",
        "    [0.3841]\n",
        "])\n",
        "\n",
        "test_data = np.array([\n",
        "    [0.5, 0.4],\n",
        "    [0.6, 0.5]\n",
        "])\n",
        "\n",
        "test_target = np.array([\n",
        "    [0.3615],\n",
        "    [0.3672]\n",
        "])\n",
        "\n",
        "nn = NeuralNetwork()\n",
        "nn.train(train_data, train_target, epochs=1000)\n",
        "\n",
        "def calculate_mse(predictions, targets):\n",
        "    return np.mean((predictions - targets) ** 2)\n",
        "\n",
        "train_predictions = nn.predict(train_data)\n",
        "train_mse = calculate_mse(train_predictions, train_target)\n",
        "print(f\"Training MSE: {train_mse:.6f}\")\n",
        "\n",
        "test_predictions = nn.predict(test_data)\n",
        "test_mse = calculate_mse(test_predictions, test_target)\n",
        "print(f\"Testing MSE: {test_mse:.6f}\")\n",
        "\n",
        "def get_user_prediction(nn):\n",
        "    try:\n",
        "        x1 = float(input(\"Enter x1 value: \"))\n",
        "        x2 = float(input(\"Enter x2 value: \"))\n",
        "        input_data = np.array([[x1, x2]])\n",
        "        prediction = nn.predict(input_data)\n",
        "        print(f\"Predicted output: {prediction[0][0]:.4f}\")\n",
        "    except ValueError:\n",
        "        print(\"Please enter valid numerical values\")\n",
        "\n",
        "print(\"\\nEnter values for prediction:\")\n",
        "get_user_prediction(nn)\n",
        "\n",
        "print(\"\\nNetwork Architecture:\")\n",
        "print(\"Input Layer: 2 neurons (x1, x2)\")\n",
        "print(\"Hidden Layer: 3 neurons (sigmoid activation)\")\n",
        "print(\"Output Layer: 1 neuron (linear activation)\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiMfdWA7LFNQ",
        "outputId": "36b1b159-c387-4e94-dd1a-a1c48841e42a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MSE: 0.000001\n",
            "Testing MSE: 0.000000\n",
            "\n",
            "Enter values for prediction:\n",
            "Enter x1 value: 0.4 \n",
            "Enter x2 value: 0.5\n",
            "Predicted output: 0.3606\n",
            "\n",
            "Network Architecture:\n",
            "Input Layer: 2 neurons (x1, x2)\n",
            "Hidden Layer: 3 neurons (sigmoid activation)\n",
            "Output Layer: 1 neuron (linear activation)\n"
          ]
        }
      ]
    }
  ]
}
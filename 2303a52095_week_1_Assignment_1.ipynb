{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7/E5b+b1ZTZ5/WR02cdqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52095/generativeAI_2025/blob/main/2303a52095_week_1_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. (1 ponto) Write Python code from scratch to find error metrics of deep learning model. Actual\n",
        " values and deep learning model predicted values are shown in Table 1. Also compare the results\n",
        " with the outcomes of libraries"
      ],
      "metadata": {
        "id": "NeoBDE13gOkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "Y_actual = np.array([20, 30, 40, 50, 60])\n",
        "Y_pred = np.array([20.5, 30.3, 40.2, 50.6, 60.7])\n",
        "n = len(Y_actual)\n",
        "mae_scratch = sum(abs(Y_actual - Y_pred)) / n\n",
        "mse_scratch = sum((Y_actual - Y_pred) ** 2) / n\n",
        "rmse_scratch = np.sqrt(mse_scratch)\n",
        "r2_scratch = 1 - (sum((Y_actual - Y_pred) ** 2) / sum((Y_actual - np.mean(Y_actual)) ** 2))\n",
        "\n",
        "print(\"Error Metrics Calculated from Scratch:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_scratch}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_scratch}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_scratch}\")\n",
        "print(f\"R-Squared (R2): {r2_scratch}\")\n",
        "\n",
        "mae_lib = mean_absolute_error(Y_actual, Y_pred)\n",
        "mse_lib = mean_squared_error(Y_actual, Y_pred)\n",
        "rmse_lib = np.sqrt(mse_lib)\n",
        "r2_lib = r2_score(Y_actual, Y_pred)\n",
        "\n",
        "print(\"\\nError Metrics Using Libraries:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_lib}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_lib}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_lib}\")\n",
        "print(f\"R-Squared (R2): {r2_lib}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF32owv_irkj",
        "outputId": "bf15375c-b8f4-4fed-d901-3d5abb674726"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error Metrics Calculated from Scratch:\n",
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n",
            "R-Squared (R2): 0.99877\n",
            "\n",
            "Error Metrics Using Libraries:\n",
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n",
            "R-Squared (R2): 0.99877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2. (1 ponto) Write python code from scratch to find evaluation metrics of deep learning model.\n",
        " Actual values and deep learning model predicted values are shown in Table 2. Also compare the\n",
        " results with outcome of libraries"
      ],
      "metadata": {
        "id": "nEN4fjlZjPC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Expected Leaning Outcomes from this assignment related to python– Students are able to understand deep learning model metrics– Students are able to write code in python to find deep learning model metrics– Students are abl"
      ],
      "metadata": {
        "id": "s2TZlDAdjiSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "Y_actual = np.array([0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 1, 1, 2, 0, 2, 0, 2, 2, 1, 0, 2, 2, 2, 2])\n",
        "Y_pred = np.array([0, 0, 0, 1, 2, 2, 1, 0, 2, 0, 2, 0, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2])\n",
        "def calculate_metrics_from_scratch(y_actual, y_pred):\n",
        "    unique_classes = np.unique(np.concatenate((y_actual, y_pred)))\n",
        "    conf_matrix = np.zeros((len(unique_classes), len(unique_classes)), dtype=int)\n",
        "    for a, p in zip(y_actual, y_pred):\n",
        "        conf_matrix[a][p] += 1\n",
        "\n",
        "    accuracy = np.trace(conf_matrix) / np.sum(conf_matrix)\n",
        "\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    for i in range(len(unique_classes)):\n",
        "        tp = conf_matrix[i][i]\n",
        "        fp = sum(conf_matrix[:, i]) - tp\n",
        "        fn = sum(conf_matrix[i, :]) - tp\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    avg_precision = np.mean(precisions)\n",
        "    avg_recall = np.mean(recalls)\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "    return accuracy, avg_precision, avg_recall, avg_f1, conf_matrix\n",
        "\n",
        "accuracy_scratch, precision_scratch, recall_scratch, f1_scratch, conf_matrix_scratch = calculate_metrics_from_scratch(Y_actual, Y_pred)\n",
        "\n",
        "print(\"Evaluation Metrics Calculated from Scratch:\")\n",
        "print(f\"Accuracy: {accuracy_scratch}\")\n",
        "print(f\"Precision (Macro-Averaged): {precision_scratch}\")\n",
        "print(f\"Recall (Macro-Averaged): {recall_scratch}\")\n",
        "print(f\"F1 Score (Macro-Averaged): {f1_scratch}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_scratch)\n",
        "\n",
        "accuracy_lib = accuracy_score(Y_actual, Y_pred)\n",
        "precision_lib = precision_score(Y_actual, Y_pred, average='macro')\n",
        "recall_lib = recall_score(Y_actual, Y_pred, average='macro')\n",
        "f1_lib = f1_score(Y_actual, Y_pred, average='macro')\n",
        "conf_matrix_lib = confusion_matrix(Y_actual, Y_pred)\n",
        "\n",
        "print(\"\\nEvaluation Metrics Using Libraries:\")\n",
        "print(f\"Accuracy: {accuracy_lib}\")\n",
        "print(f\"Precision (Macro-Averaged): {precision_lib}\")\n",
        "print(f\"Recall (Macro-Averaged): {recall_lib}\")\n",
        "print(f\"F1 Score (Macro-Averaged): {f1_lib}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_lib)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZUTKZeljmwA",
        "outputId": "c7199920-3333-4048-be7b-b54cb73b6f17"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Metrics Calculated from Scratch:\n",
            "Accuracy: 0.5833333333333334\n",
            "Precision (Macro-Averaged): 0.5158730158730158\n",
            "Recall (Macro-Averaged): 0.5148148148148147\n",
            "F1 Score (Macro-Averaged): 0.5\n",
            "Confusion Matrix:\n",
            "[[4 2 3]\n",
            " [2 1 2]\n",
            " [1 0 9]]\n",
            "\n",
            "Evaluation Metrics Using Libraries:\n",
            "Accuracy: 0.5833333333333334\n",
            "Precision (Macro-Averaged): 0.5158730158730158\n",
            "Recall (Macro-Averaged): 0.5148148148148147\n",
            "F1 Score (Macro-Averaged): 0.5\n",
            "Confusion Matrix:\n",
            "[[4 2 3]\n",
            " [2 1 2]\n",
            " [1 0 9]]\n"
          ]
        }
      ]
    }
  ]
}